{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc4f2cd-98e6-4a67-80be-3949666141cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "import os\n",
    "import fire\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "import datasets\n",
    "import numpy as np\n",
    "import argparse\n",
    "from distutils.util import strtobool\n",
    "\n",
    "from sacrebleu.metrics import BLEU\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "import minitorch\n",
    "from minitorch import DecoderLM\n",
    "from minitorch.cuda_kernel_ops import CudaKernelOps\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name, model_max_length):\n",
    "    \"\"\"\n",
    "    Obtrain IWSLT (de-en) dataset.\n",
    "    \"\"\"\n",
    "    dataset = {\n",
    "        split: datasets.load_dataset(dataset_name, split=split)['translation']\n",
    "        for split in ['train', 'validation', 'test']\n",
    "    }\n",
    "    src_key, tgt_key = 'de', 'en'\n",
    "\n",
    "    dataset = {\n",
    "        split: [\n",
    "            example for example in dataset[split]\n",
    "            if len(example[src_key].split()) + len(example[tgt_key].split()) < model_max_length\n",
    "        ] for split in dataset.keys()\n",
    "    }\n",
    "\n",
    "    dataset['test'] = dataset['test'][:100]             # 6750\n",
    "\n",
    "    print(json.dumps(\n",
    "        {'data_size': {split: len(dataset[split]) for split in dataset.keys()}},\n",
    "        indent=4))\n",
    "\n",
    "    return dataset, src_key, tgt_key\n",
    "\n",
    "\n",
    "def get_tokenizer(examples, vocab_size, src_key, tgt_key, workdir):\n",
    "    \"\"\"\n",
    "    Trains a tokenizer on the provided dataset examples and saves the tokenizer configuration.\n",
    "\n",
    "    Parameters:\n",
    "    - examples: The dataset examples used for training the tokenizer.\n",
    "    - vocab_size: The desired vocabulary size for the tokenizer.\n",
    "    - src_key: The key used to access the source text within the dataset examples.\n",
    "    - tgt_key: The key used to access the target text within the dataset examples.\n",
    "    - workdir: The directory where the tokenizer should be saved.\n",
    "\n",
    "    Returns:\n",
    "    - tokenizer: The trained tokenizer with special tokens,\n",
    "        e.g., (\"<eos_de>\", \"<eos_en>\", \"<pad>\") if src_key and tgt_key are \"de\" and \"en\", respectively.\n",
    "    \"\"\"\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    # Customized training\n",
    "    tokenizer.train_from_iterator(\n",
    "        [[example[src_key], example[tgt_key]] for example in examples],\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[f'<eos_{src_key}>', f'<eos_{tgt_key}>', '<pad>'])\n",
    "\n",
    "    tokenizer.save(f'{workdir}/tokenizer.json')\n",
    "    json.dump({'model_type': 'gpt2'}, open(f'{workdir}/config.json', 'w'))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        workdir,\n",
    "        eos_token=None,\n",
    "        bos_token=None,\n",
    "        pad_token=None,\n",
    "        unk_token=None)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def collate_batch(\n",
    "        examples, src_key, tgt_key, tokenizer, model_max_length, backend):\n",
    "    \"\"\"\n",
    "    Prepares a batch of examples for model training or evaluation by tokenizing and padding them.\n",
    "\n",
    "    Parameters:\n",
    "    - examples: A list of examples to be processed.\n",
    "    - src_key: The key for accessing source texts in the examples.\n",
    "    - tgt_key: The key for accessing target texts in the examples.\n",
    "    - tokenizer: The tokenizer to be used for encoding the texts.\n",
    "    - model_max_length: The maximum sequence length the model can handle.\n",
    "    - backend: The backend of minitorch tensors.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing keys: 'input_ids', 'labels', 'label_token_weights',\n",
    "        each indicates a minitorch tensor with shape (len(examples), model_max_length).\n",
    "\n",
    "    Notes:\n",
    "    [\"input_ids\"] for every example in the DE-EN translation, the \"input_ids\" will be:\n",
    "        <de_token_ids> + <de_eos_id> + <en_token_ids> + <en_eos_id> + <pad_ids>\n",
    "    where the pad_ids makes the length of input_ids to be model_max_length.\n",
    "\n",
    "    [\"labels\"]: the next tokens to be predicted, which will be used in the cross-entropy\n",
    "    loss function, e.g., for an example tokenized as [a, b, c, d], \"input_ids\" and \"labels\" \n",
    "    can be [a, b, c] and [b, c, d], respectively.\n",
    "\n",
    "    [\"label_token_weights\"] The 'label_token_weights' are used to differentiate\n",
    "    calculation purposes. (the MLE loss is computed on target tokens only.)\n",
    "    between the source (weight = 0) and target (weight = 1) tokens for loss\n",
    "    \"\"\"\n",
    "    token_ids, tgt_token_mask = [], []\n",
    "    max_length = model_max_length\n",
    "    pad_token_id = tokenizer.vocab['<pad>']\n",
    "    for example in examples:\n",
    "        token_ids_src = tokenizer(\n",
    "            f'{example[src_key]}<eos_{src_key}>')['input_ids']\n",
    "        token_ids_tgt = tokenizer(\n",
    "            f'{example[tgt_key]}<eos_{tgt_key}>')['input_ids']\n",
    "\n",
    "        example_token_ids = token_ids_src + token_ids_tgt\n",
    "        example_tgt_token_mask = (\n",
    "                [0] * len(token_ids_src) + [1] * len(token_ids_tgt))\n",
    "        example_token_ids = example_token_ids[:max_length]\n",
    "        example_tgt_token_mask = example_tgt_token_mask[:max_length]\n",
    "        pad_ids = [pad_token_id] * (max_length - len(example_token_ids))\n",
    "\n",
    "        token_ids.append(example_token_ids + pad_ids)\n",
    "        tgt_token_mask.append(example_tgt_token_mask + [0] * len(pad_ids))\n",
    "\n",
    "    # TODO: make examples in a 1d list, provide shape to initialize minitorch.Tensor\n",
    "    token_ids = np.array(token_ids)\n",
    "    tgt_token_mask = np.array(tgt_token_mask)\n",
    "\n",
    "    input_ids = token_ids[:, :-1]\n",
    "    labels    = token_ids[:, 1:]\n",
    "    label_token_weights = tgt_token_mask[:, 1:]\n",
    "\n",
    "    input_ids = minitorch.tensor_from_numpy(input_ids, backend=backend)\n",
    "    labels    = minitorch.tensor_from_numpy(labels, backend=backend)\n",
    "    label_token_weights = minitorch.tensor_from_numpy(label_token_weights, backend=backend)\n",
    "    \n",
    "    # input_ids = token_ids[:, :-1].tolist()\n",
    "    # labels    = token_ids[:, 1:].tolist()\n",
    "    # label_token_weights = tgt_token_mask[:, 1:].tolist()\n",
    "\n",
    "    # input_ids = minitorch.tensor(input_ids, backend=backend)\n",
    "    # labels    = minitorch.tensor(labels, backend=backend)\n",
    "    # label_token_weights = minitorch.tensor(label_token_weights, backend=backend)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "        'label_token_weights': label_token_weights\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(batch, model):\n",
    "    \"\"\"\n",
    "    The MLE loss for a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - batch: The result of collate_fn, a dict with \"input_ids\", \"labels\", and \"label_token_weights\".\n",
    "    - model: The model to be trained.\n",
    "\n",
    "    Returns:\n",
    "    - A scalar loss value for this batch, averaged across all target tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    idx = batch['input_ids']\n",
    "    idx.requires_grad_(True)\n",
    "    # print(\"getting into loss_fn\")\n",
    "    logits = model(idx=idx)\n",
    "    # print(\"finish prediction\")\n",
    "    bs, l, c = logits.shape\n",
    "    logits = logits.view(bs * l, c)\n",
    "    targets = batch['labels'].view(bs * l)\n",
    "    label_token_weights = batch['label_token_weights'].view(bs * l)\n",
    "\n",
    "    targets.requires_grad_(True)\n",
    "    # print(\"start calculating loss\")\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    loss = minitorch.nn.softmax_loss(\n",
    "        logits=logits,\n",
    "        target=targets\n",
    "    )\n",
    "\n",
    "    return ((loss * label_token_weights).sum() / label_token_weights.sum())\n",
    "\n",
    "\n",
    "def train(model, optimizer, examples, n_samples, collate_fn, batch_size, desc):\n",
    "    model.train()\n",
    "    random.shuffle(examples)\n",
    "    examples = examples[:n_samples]\n",
    "\n",
    "    for i in (prog_bar := tqdm.trange(\n",
    "            0, len(examples), batch_size, desc=f'Training ({desc})')):\n",
    "        \n",
    "        batch = collate_fn(examples=examples[i:i + batch_size])\n",
    "        \n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(batch=batch, model=model)\n",
    "        t1 = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        t2 = time.time()\n",
    "\n",
    "        optimizer.step()\n",
    "        t3 = time.time()\n",
    "\n",
    "        # print(f\"Forward: {t1 - t0}\")\n",
    "        # print(f\"Backward: {t2 - t1}\")\n",
    "        # print(f\"Opt.step: {t3 - t2}\")\n",
    "\n",
    "        batch_time = time.time() - t0\n",
    "        prog_bar.set_postfix(\n",
    "            tokens_per_sec=np.prod(batch['input_ids'].shape) / batch_time,\n",
    "            loss=loss.item(),\n",
    "            lr=optimizer.lr)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    def str2bool(x):\n",
    "        return bool(strtobool(x))\n",
    "        \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--use-fused-kernel', type=str2bool, default=False)\n",
    "    return parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127b25c4-a84c-4bbf-ac75-ac1136eda2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/alien/.cache/huggingface/modules/datasets_modules/datasets/iwslt14-de-en-preprocess/c16e61361ef4d92649321b04532f0d57c5a783ad7fd32afe4a9649a6b6107f8a (last modified on Mon May  5 21:12:05 2025) since it couldn't be found locally at iwslt14-de-en-preprocess., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset iwslt14-de-en-preprocess (/home/alien/.cache/huggingface/datasets/iwslt14-de-en-preprocess/de-en/1.0.0/c16e61361ef4d92649321b04532f0d57c5a783ad7fd32afe4a9649a6b6107f8a)\n",
      "Using the latest cached version of the module from /home/alien/.cache/huggingface/modules/datasets_modules/datasets/iwslt14-de-en-preprocess/c16e61361ef4d92649321b04532f0d57c5a783ad7fd32afe4a9649a6b6107f8a (last modified on Mon May  5 21:12:05 2025) since it couldn't be found locally at iwslt14-de-en-preprocess., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset iwslt14-de-en-preprocess (/home/alien/.cache/huggingface/datasets/iwslt14-de-en-preprocess/de-en/1.0.0/c16e61361ef4d92649321b04532f0d57c5a783ad7fd32afe4a9649a6b6107f8a)\n",
      "Using the latest cached version of the module from /home/alien/.cache/huggingface/modules/datasets_modules/datasets/iwslt14-de-en-preprocess/c16e61361ef4d92649321b04532f0d57c5a783ad7fd32afe4a9649a6b6107f8a (last modified on Mon May  5 21:12:05 2025) since it couldn't be found locally at iwslt14-de-en-preprocess., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset iwslt14-de-en-preprocess (/home/alien/.cache/huggingface/datasets/iwslt14-de-en-preprocess/de-en/1.0.0/c16e61361ef4d92649321b04532f0d57c5a783ad7fd32afe4a9649a6b6107f8a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data_size\": {\n",
      "        \"train\": 97976,\n",
      "        \"validation\": 4512,\n",
      "        \"test\": 100\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_name='iwslt14-de-en-preprocess'\n",
    "model_max_length=40\n",
    "n_epochs=1\n",
    "batch_size=128\n",
    "learning_rate=0.02\n",
    "samples_per_epoch=128\n",
    "n_vocab=10000\n",
    "n_embd=256\n",
    "seed=11111         \n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "workdir = f'./workdir_vocab{n_vocab}_lr{learning_rate}_embd{n_embd}'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "backend = minitorch.TensorBackend(CudaKernelOps)\n",
    "\n",
    "config = {\n",
    "    'n_vocab'     : n_vocab,  # vocab_size\n",
    "    'n_embd'      : n_embd,   # n_embed\n",
    "    'n_head'      : 8,    # n_head\n",
    "    'n_positions' : model_max_length,  # n_ctx == n_positions\n",
    "    # 'n_layer'     : 4,    # n_layer\n",
    "    'p_dropout'   : 0.1,  # x_pdrop\n",
    "    'ln_eps'      : 1e-5, # layer_norm_epsilon\n",
    "    'backend'     : backend,\n",
    "    'use_fused_kernel': True\n",
    "}\n",
    "\n",
    "model = DecoderLM(**config)\n",
    "optimizer = minitorch.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "dataset, src_key, tgt_key = get_dataset(\n",
    "    dataset_name=dataset_name, model_max_length=model_max_length)\n",
    "\n",
    "tokenizer = get_tokenizer(\n",
    "    examples=dataset['train'],\n",
    "    vocab_size=config['n_vocab'],\n",
    "    src_key=src_key,\n",
    "    tgt_key=tgt_key,\n",
    "    workdir=workdir)\n",
    "\n",
    "collate_fn = partial(\n",
    "    collate_batch,\n",
    "    src_key=src_key,\n",
    "    tgt_key=tgt_key,\n",
    "    tokenizer=tokenizer,\n",
    "    model_max_length=model_max_length,\n",
    "    backend=backend)\n",
    "\n",
    "\n",
    "examples=dataset['train']\n",
    "n_samples=samples_per_epoch\n",
    "model.train()\n",
    "random.shuffle(examples)\n",
    "examples = examples[:n_samples]\n",
    "\n",
    "batch = collate_fn(examples=examples[0:0 + batch_size])\n",
    "\n",
    "optimizer.zero_grad()\n",
    "# myloss = loss_fn(batch=batch, model=model)\n",
    "\n",
    "idx = batch['input_ids']\n",
    "idx.requires_grad_(True)\n",
    "# print(\"getting into loss_fn\")\n",
    "logits = model(idx=idx)\n",
    "# print(\"finish prediction\")\n",
    "bs, l, c = logits.shape\n",
    "logits = logits.view(bs * l, c)\n",
    "targets = batch['labels'].view(bs * l)\n",
    "label_token_weights = batch['label_token_weights'].view(bs * l)\n",
    "\n",
    "targets.requires_grad_(True)\n",
    "# print(\"start calculating loss\")\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "loss = minitorch.nn.softmax_loss(\n",
    "    logits=logits,\n",
    "    target=targets\n",
    ")\n",
    "\n",
    "myloss= ((loss * label_token_weights).sum() / label_token_weights.sum())\n",
    "\n",
    "\n",
    "myloss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2ba289-c8e1-4aa5-b9dc-645c432b067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[9.345477]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16fbaf3-89fa-414c-8260-c9f2f56ad9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050e9b0-41d8-4e89-8a9d-ac30276c9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece1c24-6644-44fd-aa34-81ae536024aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68e4c74-6820-45b4-908f-64308d1a7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfusedmodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f95c6d4-4c7d-4528-aa05-273cf773c907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.998000 0.998000 0.998001 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998001 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.001997 0.998001 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 1.002000 0.998000 0.998000 0.998000 0.998001 0.998002 0.998000 1.002000 0.998000 1.001999 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998001 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 1.002000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 1.002000 0.998000 1.002000 1.002000 0.998000 1.001987 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.001997 0.998000 0.998000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 0.998000 1.002000 1.002000 0.998000 0.998000 0.998000 1.001999 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 0.998006 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.001999 0.998001 0.998000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.001999 0.998002 0.998000 1.002000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 1.002000 0.998001 1.002000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 1.002000 0.998000 1.002000 0.998000 0.998000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 1.002000 1.002000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998000 0.998008 0.998000 0.998000 1.002000 0.998000 1.002000 0.998000 0.998000 0.998000]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfusedmodel.ln.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b361fa2-dd34-4b49-83a5-0f45073f5ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ln.weights.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07e767-3248-4d61-a262-f63cdfc1b42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
